{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9293f5-c434-4f35-9bba-3beb989c7ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_key</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label_code</th>\n",
       "      <th>preprocessed_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject24_top</td>\n",
       "      <td>24</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject24_middle</td>\n",
       "      <td>24</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject24_bottom</td>\n",
       "      <td>24</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject9_top</td>\n",
       "      <td>9</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject9_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject9_middle</td>\n",
       "      <td>9</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject9_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          slice_key  subject_id label_name  label_code  \\\n",
       "0     subject24_top          24         NT           1   \n",
       "1  subject24_middle          24         NT           1   \n",
       "2  subject24_bottom          24         NT           1   \n",
       "3      subject9_top           9         NT           1   \n",
       "4   subject9_middle           9         NT           1   \n",
       "\n",
       "                                   preprocessed_path  \n",
       "0  Data/processed/sorensen/preprocessed/subject24...  \n",
       "1  Data/processed/sorensen/preprocessed/subject24...  \n",
       "2  Data/processed/sorensen/preprocessed/subject24...  \n",
       "3  Data/processed/sorensen/preprocessed/subject9_...  \n",
       "4  Data/processed/sorensen/preprocessed/subject9_...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT   = Path(\"Data\")\n",
    "OUT         = DATA_ROOT / \"processed\" / \"sorensen\"\n",
    "MANIFEST_CLEAN = Path(\"Data/processed/emphysema_all/manifest_emphysema_all.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(MANIFEST_CLEAN)\n",
    "cols_keep = [\"slice_key\",\"subject_id\",\"label_name\",\"label_code\",\"preprocessed_path\"]\n",
    "df = df[cols_keep].copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4e4d6b-1b89-497b-8d35-96513f8f6557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: SPECIAL (only 2 PLE subjects). PATH_COL=preprocessed_path\n",
      "WARNING: VAL chứa 1 slice PLE từ subject 25. Chỉ dùng VAL cho early-stopping; đánh giá tổng quát PLE dựa trên TEST.\n",
      "Số lượng: {'train': 54, 'val': 19, 'test': 42}\n",
      "Số subject: {'train': 19, 'val': 7, 'test': 14}\n",
      "\n",
      "Phân bố lớp (train):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NT</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLE</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSE</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLE</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "label_name       \n",
       "NT             31\n",
       "CLE             9\n",
       "PSE            12\n",
       "PLE             2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phân bố lớp (val):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NT</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLE</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSE</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLE</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "label_name       \n",
       "NT              3\n",
       "CLE             3\n",
       "PSE            12\n",
       "PLE             1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phân bố lớp (test):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NT</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLE</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSE</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLE</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "label_name       \n",
       "NT             27\n",
       "CLE             9\n",
       "PSE             3\n",
       "PLE             3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2) Split theo subject (grouped) + stratified theo label\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# ---- 0) Cột định danh file/slice dùng để so khớp loại trừ ----\n",
    "PATH_COL = (\n",
    "    \"image_path\" if \"image_path\" in df.columns\n",
    "    else (\"preprocessed_path\" if \"preprocessed_path\" in df.columns\n",
    "          else (\"slice_key\" if \"slice_key\" in df.columns else None))\n",
    ")\n",
    "if PATH_COL is None:\n",
    "    raise ValueError(\"Không tìm thấy cột định danh (image_path / preprocessed_path / slice_key) trong df.\")\n",
    "\n",
    "# ---- 1) Thứ tự classes (nếu chưa có) ----\n",
    "try:\n",
    "    classes\n",
    "except NameError:\n",
    "    classes = [\"NT\",\"CLE\",\"PSE\",\"PLE\"] if {\"NT\",\"CLE\",\"PSE\",\"PLE\"}.issubset(set(df[\"label_name\"].unique())) \\\n",
    "              else sorted(df[\"label_name\"].unique().tolist())\n",
    "\n",
    "# ---- 2) Major label theo subject để stratify ----\n",
    "subject_stats = df.groupby([\"subject_id\",\"label_name\"]).size().unstack(fill_value=0)\n",
    "major_label   = subject_stats.idxmax(axis=1)\n",
    "sub2label     = major_label.to_dict()\n",
    "\n",
    "# Các subject có PLE và số slice PLE theo subject\n",
    "ple_subj         = subject_stats.index[subject_stats.get(\"PLE\", 0) > 0].tolist()\n",
    "ple_cnt_by_subj  = subject_stats.get(\"PLE\", 0)\n",
    "\n",
    "# ---- 3) Helpers ----\n",
    "def safe_n_splits(labels, desired, min_allowed=2):\n",
    "    \"\"\"Giảm n_splits không vượt quá số lượng lớp ít nhất.\"\"\"\n",
    "    vc = pd.Series(labels).value_counts()\n",
    "    return max(min_allowed, min(desired, int(vc.min())))\n",
    "\n",
    "def _move_subject(src, dst, sid):\n",
    "    rows = src[src[\"subject_id\"]==sid]\n",
    "    src  = src[src[\"subject_id\"]!=sid].reset_index(drop=True)\n",
    "    dst  = pd.concat([dst, rows], ignore_index=True)\n",
    "    return src, dst\n",
    "\n",
    "def _save_and_report(train_df, val_df, test_df):\n",
    "    print(\"Số lượng:\", { \"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df) })\n",
    "    print(\"Số subject:\", {\n",
    "        \"train\": train_df[\"subject_id\"].nunique(),\n",
    "        \"val\":   val_df[\"subject_id\"].nunique(),\n",
    "        \"test\":  test_df[\"subject_id\"].nunique()\n",
    "    })\n",
    "    # Lưu CSV chia tập\n",
    "    SPLIT_DIR = OUT / \"splits\"\n",
    "    SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    train_df.to_csv(SPLIT_DIR / \"train.csv\", index=False)\n",
    "    val_df.to_csv(SPLIT_DIR / \"val.csv\", index=False)\n",
    "    test_df.to_csv(SPLIT_DIR / \"test.csv\", index=False)\n",
    "    # Phân bố lớp\n",
    "    print(\"\\nPhân bố lớp (train):\")\n",
    "    display(train_df[\"label_name\"].value_counts().reindex(classes, fill_value=0).to_frame(\"count\"))\n",
    "    print(\"\\nPhân bố lớp (val):\")\n",
    "    display(val_df[\"label_name\"].value_counts().reindex(classes, fill_value=0).to_frame(\"count\"))\n",
    "    print(\"\\nPhân bố lớp (test):\")\n",
    "    display(test_df[\"label_name\"].value_counts().reindex(classes, fill_value=0).to_frame(\"count\"))\n",
    "\n",
    "# =========================\n",
    "# CASE A: >= 3 PLE subjects\n",
    "# =========================\n",
    "groups  = df[\"subject_id\"].values\n",
    "y_major = df[\"subject_id\"].map(sub2label).values\n",
    "\n",
    "if len(ple_subj) >= 3:\n",
    "    n_splits1 = safe_n_splits(y_major, desired=5, min_allowed=2)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits1, shuffle=True, random_state=42)\n",
    "    fold_pairs = list(sgkf.split(df, y=y_major, groups=groups))\n",
    "\n",
    "    test_idx   = fold_pairs[0][1]\n",
    "    remain_idx = fold_pairs[0][0]\n",
    "    df_remain  = df.iloc[remain_idx].copy()\n",
    "\n",
    "    groups_r  = df_remain[\"subject_id\"].values\n",
    "    y_major_r = df_remain[\"subject_id\"].map(sub2label).values\n",
    "    n_splits2 = safe_n_splits(y_major_r, desired=4, min_allowed=2)\n",
    "\n",
    "    sgkf2     = StratifiedGroupKFold(n_splits=n_splits2, shuffle=True, random_state=7)\n",
    "    val_idx_r, train_idx_r = list(sgkf2.split(df_remain, y=y_major_r, groups=groups_r))[0][1], \\\n",
    "                             list(sgkf2.split(df_remain, y=y_major_r, groups=groups_r))[0][0]\n",
    "\n",
    "    train_df = df_remain.iloc[train_idx_r].reset_index(drop=True)\n",
    "    val_df   = df_remain.iloc[val_idx_r].reset_index(drop=True)\n",
    "    test_df  = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Sửa sau chia để đảm bảo mỗi tập có đủ lớp\n",
    "    subj_cls = df.groupby([\"subject_id\",\"label_name\"]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Ưu tiên TEST rồi VAL\n",
    "    for cls in classes:\n",
    "        if cls not in test_df[\"label_name\"].unique():\n",
    "            cands = (subj_cls[subj_cls[cls]>0][cls].sort_values(ascending=False).index.tolist())\n",
    "            cands = [sid for sid in cands if sid in set(train_df[\"subject_id\"].unique())]\n",
    "            if cands:\n",
    "                train_df, test_df = _move_subject(train_df, test_df, cands[0])\n",
    "    for cls in classes:\n",
    "        if cls not in val_df[\"label_name\"].unique():\n",
    "            cands = (subj_cls[subj_cls[cls]>0][cls].sort_values(ascending=False).index.tolist())\n",
    "            cands = [sid for sid in cands if sid in set(train_df[\"subject_id\"].unique())]\n",
    "            if cands:\n",
    "                train_df, val_df = _move_subject(train_df, val_df, cands[0])\n",
    "\n",
    "    # Loại trùng subject\n",
    "    s_test = set(test_df[\"subject_id\"].unique())\n",
    "    s_val  = set(val_df[\"subject_id\"].unique())\n",
    "    train_df = train_df[~train_df[\"subject_id\"].isin(s_test | s_val)].reset_index(drop=True)\n",
    "    val_df   = val_df[~val_df[\"subject_id\"].isin(s_test)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Mode: regular (>=3 PLE subjects). PATH_COL={PATH_COL}\")\n",
    "    _save_and_report(train_df, val_df, test_df)\n",
    "\n",
    "# =========================\n",
    "# CASE B: chỉ có 2 PLE subjects\n",
    "# =========================\n",
    "else:\n",
    "    print(f\"Mode: SPECIAL (only 2 PLE subjects). PATH_COL={PATH_COL}\")\n",
    "\n",
    "    # Chọn subject PLE có ít slice hơn cho TEST để giữ nhiều PLE cho train/val\n",
    "    ple_subj_sorted = sorted(ple_subj, key=lambda s: int(ple_cnt_by_subj.loc[s]))\n",
    "    ple_test_sid, ple_train_sid = ple_subj_sorted[0], ple_subj_sorted[1]\n",
    "\n",
    "    # 1) TEST: trọn subject ple_test_sid + thêm fold từ phần còn lại\n",
    "    test_df_ple = df[df[\"subject_id\"]==ple_test_sid]\n",
    "    df_non_test = df[df[\"subject_id\"]!=ple_test_sid].reset_index(drop=True)\n",
    "\n",
    "    groups_nt  = df_non_test[\"subject_id\"].values\n",
    "    y_major_nt = df_non_test[\"subject_id\"].map(sub2label).values\n",
    "    n_splits_nt = safe_n_splits(y_major_nt, desired=5, min_allowed=2)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits_nt, shuffle=True, random_state=42)\n",
    "    folds = list(sgkf.split(df_non_test, y=y_major_nt, groups=groups_nt))\n",
    "    extra_test_idx = folds[0][1]\n",
    "\n",
    "    test_df_rest = df_non_test.iloc[extra_test_idx].reset_index(drop=True)\n",
    "    test_df      = pd.concat([test_df_ple, test_df_rest], ignore_index=True)\n",
    "\n",
    "    # 2) Pool còn lại cho TRAIN/VAL (loại bỏ các hàng đã vào TEST theo PATH_COL)\n",
    "    df_pool = df[~df[PATH_COL].isin(test_df[PATH_COL])].reset_index(drop=True)\n",
    "\n",
    "    # 3) Với subject PLE còn lại: CHIA THEO SLICE cho VAL (>=1) và giữ LẠI >=1 cho TRAIN nếu có thể\n",
    "    pool_ple  = df_pool[df_pool[\"subject_id\"]==ple_train_sid].reset_index(drop=True)\n",
    "    pool_rest = df_pool[df_pool[\"subject_id\"]!=ple_train_sid].reset_index(drop=True)\n",
    "\n",
    "    # số slice PLE còn lại (chỉ 1 subject)\n",
    "    n_ple_total = len(pool_ple)\n",
    "    # chọn số slice cho VAL: tối thiểu 1, tối đa n_ple_total-1 (để còn lại >=1 cho TRAIN nếu n_ple_total>1)\n",
    "    n_val_ple = 1 if n_ple_total >= 1 else 0\n",
    "    if n_ple_total > 1:\n",
    "        n_val_ple = min(max(1, int(round(n_ple_total * 0.15))), n_ple_total - 1)\n",
    "    # (nếu chỉ có đúng 1 slice thì val = 1, train = 0 – trường hợp bất khả kháng)\n",
    "\n",
    "    if n_val_ple > 0:\n",
    "        rng = np.random.RandomState(123)\n",
    "        val_ple_idx = rng.choice(n_ple_total, size=n_val_ple, replace=False)\n",
    "        val_ple_df  = pool_ple.iloc[val_ple_idx]\n",
    "        train_ple_df= pool_ple.drop(pool_ple.index[val_ple_idx])\n",
    "    else:\n",
    "        val_ple_df  = pool_ple.iloc[:0]\n",
    "        train_ple_df= pool_ple.copy()\n",
    "\n",
    "    # 4) Non-PLE: chia theo subject cho TRAIN/VAL\n",
    "    groups_r  = pool_rest[\"subject_id\"].values\n",
    "    y_major_r = pool_rest[\"subject_id\"].map(sub2label).values\n",
    "    n_splits2 = safe_n_splits(y_major_r, desired=4, min_allowed=2)\n",
    "    sgkf2     = StratifiedGroupKFold(n_splits=n_splits2, shuffle=True, random_state=7)\n",
    "    _split    = list(sgkf2.split(pool_rest, y=y_major_r, groups=groups_r))[0]\n",
    "    val_idx_r, train_idx_r = _split[1], _split[0]\n",
    "\n",
    "    val_rest_df   = pool_rest.iloc[val_idx_r].reset_index(drop=True)\n",
    "    train_rest_df = pool_rest.iloc[train_idx_r].reset_index(drop=True)\n",
    "\n",
    "    # GHÉP: cho phép trùng subject ple_train_sid giữa TRAIN và VAL (slice-level split)\n",
    "    val_df   = pd.concat([val_rest_df,   val_ple_df],   ignore_index=True)\n",
    "    train_df = pd.concat([train_rest_df, train_ple_df], ignore_index=True)\n",
    "\n",
    "    # Loại trùng subject với TEST (đảm bảo disjoint theo subject so với TEST)\n",
    "    s_test = set(test_df[\"subject_id\"].unique())\n",
    "    train_df = train_df[~train_df[\"subject_id\"].isin(s_test)].reset_index(drop=True)\n",
    "    val_df   = val_df[~val_df[\"subject_id\"].isin(s_test)].reset_index(drop=True)\n",
    "\n",
    "    # KHÔNG loại trùng subject giữa TRAIN và VAL đối với ple_train_sid (cho phép overlap slice-level)\n",
    "    # Nếu có subject trùng khác ngoài ple_train_sid (hiếm), loại khỏi TRAIN để tránh leakage\n",
    "    overlap = (set(train_df[\"subject_id\"].unique()) & set(val_df[\"subject_id\"].unique())) - {ple_train_sid}\n",
    "    if overlap:\n",
    "        train_df = train_df[~train_df[\"subject_id\"].isin(overlap)].reset_index(drop=True)\n",
    "\n",
    "    # Cảnh báo leakage ở VAL do lấy slice từ subject PLE còn lại\n",
    "    print(f\"WARNING: VAL chứa {(val_df['label_name']=='PLE').sum()} slice PLE từ subject {ple_train_sid}. \"\n",
    "          \"Chỉ dùng VAL cho early-stopping; đánh giá tổng quát PLE dựa trên TEST.\")\n",
    "\n",
    "    _save_and_report(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa7939a-83a8-48c1-9a8f-89c776f3bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NT': 0.43548387096774194, 'CLE': 1.5, 'PSE': 1.125, 'PLE': 6.75}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Class weights theo phân bố train\n",
    "from collections import Counter\n",
    "\n",
    "classes = [\"NT\",\"CLE\",\"PSE\",\"PLE\"]  \n",
    "cnt = Counter(train_df[\"label_name\"])\n",
    "N = sum(cnt.values()); K = len(classes)\n",
    "class_weights = {c: (N / (K * cnt.get(c,1))) for c in classes}\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb0b491-5ade-4901-835d-b4a1ed5e5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dataset + Augmentation\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "\n",
    "label2id = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "train_tfms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05,\n",
    "                       rotate_limit=8, border_mode=0, value=0, p=0.5),\n",
    "    A.ElasticTransform(alpha=10, sigma=10*0.05, alpha_affine=5,\n",
    "                       border_mode=0, value=0, p=0.15),\n",
    "], p=1.0)\n",
    "\n",
    "val_tfms = A.Compose([], p=1.0)\n",
    "\n",
    "class SorensenNPZDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        z = np.load(row[\"preprocessed_path\"], allow_pickle=True)\n",
    "        img = z[\"img_norm\"].astype(np.float32)  # [H,W] in [0,1]\n",
    "        \n",
    "\n",
    "        # Albumentations nhận HxWxc -> thêm kênh giả\n",
    "        img3 = np.expand_dims(img, axis=2)\n",
    "        if self.transforms is not None:\n",
    "            img3 = self.transforms(image=img3)[\"image\"]\n",
    "        img = img3[...,0]  # quay lại HxW\n",
    "\n",
    "        # To tensor (1,H,W)\n",
    "        x = torch.from_numpy(img).unsqueeze(0)        # 1 channel\n",
    "        y = torch.tensor(label2id[row[\"label_name\"]], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d47a433-1595-474f-8669-3ceaa03dcce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4355, 1.5000, 1.1250, 6.7500])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) DataLoader\n",
    "import os\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0  # max(0, min(4, os.cpu_count() // 2))\n",
    "\n",
    "train_ds = SorensenNPZDataset(train_df, transforms=train_tfms)\n",
    "val_ds   = SorensenNPZDataset(val_df,   transforms=val_tfms)\n",
    "test_ds  = SorensenNPZDataset(test_df,  transforms=val_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Class weights tensor cho CrossEntropy\n",
    "weights_tensor = torch.tensor([class_weights[c] for c in classes], dtype=torch.float32)\n",
    "weights_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0483449a-e9fe-4274-8da1-6bf3c92cab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\QUANG/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [07:18<00:00, 107kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "ResNet                                   [1, 1, 512, 512]          [1, 4]                    --\n",
       "├─Conv2d: 1-1                            [1, 1, 512, 512]          [1, 64, 256, 256]         3,136\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 256, 256]         [1, 64, 256, 256]         128\n",
       "├─ReLU: 1-3                              [1, 64, 256, 256]         [1, 64, 256, 256]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 256, 256]         [1, 64, 128, 128]         --\n",
       "├─Sequential: 1-5                        [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 128, 128]         [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 128, 128]         [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 128, 128]         [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 128, 128]         [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 128, 128]         [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 128, 128]         [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 128, 128]         [1, 64, 128, 128]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 128, 128]         [1, 64, 128, 128]         128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 128, 128]         [1, 64, 128, 128]         --\n",
       "├─Sequential: 1-6                        [1, 64, 128, 128]         [1, 128, 64, 64]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 64, 128, 128]         [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 64, 128, 128]         [1, 128, 64, 64]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 64, 64]          [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 64, 64]          [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 64, 64]          [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 64, 64]          [1, 128, 64, 64]          256\n",
       "│    │    └─Sequential: 3-18             [1, 64, 128, 128]         [1, 128, 64, 64]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 64, 64]          [1, 128, 64, 64]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 64, 64]          [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 64, 64]          [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 64, 64]          [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 64, 64]          [1, 128, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 64, 64]          [1, 128, 64, 64]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 64, 64]          [1, 128, 64, 64]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 64, 64]          [1, 128, 64, 64]          --\n",
       "├─Sequential: 1-7                        [1, 128, 64, 64]          [1, 256, 32, 32]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 128, 64, 64]          [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 128, 64, 64]          [1, 256, 32, 32]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 32, 32]          [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 32, 32]          [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 32, 32]          [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 32, 32]          [1, 256, 32, 32]          512\n",
       "│    │    └─Sequential: 3-31             [1, 128, 64, 64]          [1, 256, 32, 32]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 32, 32]          [1, 256, 32, 32]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 32, 32]          [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 32, 32]          [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 32, 32]          [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 32, 32]          [1, 256, 32, 32]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 32, 32]          [1, 256, 32, 32]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 32, 32]          [1, 256, 32, 32]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 32, 32]          [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-8                        [1, 256, 32, 32]          [1, 512, 16, 16]          --\n",
       "│    └─BasicBlock: 2-7                   [1, 256, 32, 32]          [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 256, 32, 32]          [1, 512, 16, 16]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 16, 16]          [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 16, 16]          [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 16, 16]          [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 16, 16]          [1, 512, 16, 16]          1,024\n",
       "│    │    └─Sequential: 3-44             [1, 256, 32, 32]          [1, 512, 16, 16]          132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 16, 16]          [1, 512, 16, 16]          --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 16, 16]          [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 16, 16]          [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 16, 16]          [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 16, 16]          [1, 512, 16, 16]          --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 16, 16]          [1, 512, 16, 16]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 16, 16]          [1, 512, 16, 16]          1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 16, 16]          [1, 512, 16, 16]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 16, 16]          [1, 512, 1, 1]            --\n",
       "├─Linear: 1-10                           [1, 512]                  [1, 4]                    2,052\n",
       "===================================================================================================================\n",
       "Total params: 11,172,292\n",
       "Trainable params: 11,172,292\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 9.06\n",
       "===================================================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 207.62\n",
       "Params size (MB): 44.69\n",
       "Estimated Total Size (MB): 253.36\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model: ResNet18\n",
    "import torch, torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "classes = [\"NT\",\"CLE\",\"PSE\",\"PLE\"]\n",
    "\n",
    "def build_resnet18(num_classes=4, in_ch=1):\n",
    "    model = models.resnet18(\n",
    "        weights=models.ResNet18_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    # sửa conv1 nhận 1 kênh\n",
    "    if in_ch == 1:\n",
    "        old_conv = model.conv1\n",
    "        model.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "    # thay FC head\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = build_resnet18(num_classes=len(classes), in_ch=1).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 1, 512, 512),\n",
    "        col_names=(\"input_size\",\"output_size\",\"num_params\"),\n",
    "        depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44292d99-2b16-483c-861f-07d22eb0b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# dùng CrossEntropy với class_weights\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights_tensor.to(device),\n",
    "    label_smoothing=0.05\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "BEST_PATH = OUT / \"resnet18_best.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca117d43-6d1c-4fe7-8057-94fc51abc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model, criterion, device, classes):\n",
    "    model.eval()\n",
    "    tot_loss, n = 0.0, 0\n",
    "    probs_all, y_all = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tot_loss += loss.item() * x.size(0); n += x.size(0)\n",
    "        probs_all.append(logits.softmax(1).cpu().numpy())\n",
    "        y_all.append(y.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_all)\n",
    "    y_prob = np.concatenate(probs_all, axis=0)\n",
    "    y_pred = y_prob.argmax(1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    # AUC OvR theo lớp khả dụng (không NaN)\n",
    "    per_class_auc, auc_vals = {}, []\n",
    "    C = len(classes)\n",
    "    for i, cls in enumerate(classes):\n",
    "        y_bin = (y_true == i).astype(int)\n",
    "        if y_bin.min() == y_bin.max():  # thiếu lớp i ở tập này\n",
    "            per_class_auc[cls] = np.nan\n",
    "        else:\n",
    "            auc_i = roc_auc_score(y_bin, y_prob[:, i])\n",
    "            per_class_auc[cls] = float(auc_i); auc_vals.append(auc_i)\n",
    "    macro_auc = float(np.mean(auc_vals)) if len(auc_vals) > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"loss\": tot_loss / max(n, 1),\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": macro_auc,\n",
    "        \"per_class_auc\": per_class_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb87a71-7c1a-423f-8081-2a6891037df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\NCKH\\mlenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/30 - loss: 1.4741 - val_loss: 1.4441 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.4931\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.4931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/30 - loss: 1.0812 - val_loss: 1.9084 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.7339\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.7339)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/30 - loss: 0.9119 - val_loss: 2.5806 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.6930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/30 - loss: 0.8884 - val_loss: 2.8811 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/30 - loss: 0.8060 - val_loss: 3.0425 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.7366\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.7366)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/30 - loss: 0.6789 - val_loss: 3.4808 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.7552\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.7552)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07/30 - loss: 0.8090 - val_loss: 3.3596 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.7378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/30 - loss: 0.7535 - val_loss: 2.6196 - val_acc: 0.1579 - val_f1: 0.0682 - val_auc: 0.6935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09/30 - loss: 0.5951 - val_loss: 2.3011 - val_acc: 0.2105 - val_f1: 0.1964 - val_auc: 0.7145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - loss: 0.6390 - val_loss: 2.0607 - val_acc: 0.2632 - val_f1: 0.2635 - val_auc: 0.7212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - loss: 0.6573 - val_loss: 1.8998 - val_acc: 0.3684 - val_f1: 0.4030 - val_auc: 0.7242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - loss: 0.7000 - val_loss: 1.7131 - val_acc: 0.4737 - val_f1: 0.4426 - val_auc: 0.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - loss: 0.6723 - val_loss: 1.5504 - val_acc: 0.5263 - val_f1: 0.5056 - val_auc: 0.7453\n",
      "Early stopping triggered.\n",
      "\n",
      "=== TEST ===\n",
      "loss=1.8401 | acc=0.643 | macro-F1=0.196 | macro-AUC=0.541\n",
      "AUC theo lớp: {'NT': 0.45432098765432094, 'CLE': 0.7171717171717172, 'PSE': 0.4786324786324786, 'PLE': 0.5128205128205128}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Cấu hình\n",
    "EPOCHS = 30                      \n",
    "BEST_PATH = OUT / \"resnet18_best.pth\"\n",
    "EARLYSTOP_METRIC = \"auc\"         # \"auc\" hoặc \"f1\"\n",
    "early_patience = 7\n",
    "\n",
    "# AMP (API mới, sửa cảnh báo)\n",
    "scaler = GradScaler('cuda' if device.type=='cuda' else 'cpu')\n",
    "\n",
    "best_score, bad_count = -1.0, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running, n_seen = 0.0, 0\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False, ncols=100)\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast('cuda' if device.type=='cuda' else 'cpu'):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        running += loss.item() * x.size(0); n_seen += x.size(0)\n",
    "        pbar.set_postfix_str(f\"loss={running/max(n_seen,1):.4f}\")\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    train_loss = running / max(n_seen, 1)\n",
    "    val_metrics = evaluate(val_loader, model, criterion, device, classes)\n",
    "    scheduler.step()\n",
    "\n",
    "    # log tóm tắt \n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} \"\n",
    "          f\"- loss: {train_loss:.4f} \"\n",
    "          f\"- val_loss: {val_metrics['loss']:.4f} \"\n",
    "          f\"- val_acc: {val_metrics['acc']:.4f} \"\n",
    "          f\"- val_f1: {val_metrics['f1']:.4f} \"\n",
    "          f\"- val_auc: {val_metrics['auc']:.4f}\")\n",
    "\n",
    "    # Early stopping theo AUC (fallback F1 nếu AUC nan)\n",
    "    es_metric = (val_metrics[\"auc\"] if (EARLYSTOP_METRIC==\"auc\" and not np.isnan(val_metrics[\"auc\"]))\n",
    "                 else val_metrics[\"f1\"])\n",
    "    if es_metric > best_score + 1e-4:\n",
    "        best_score = es_metric; bad_count = 0\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": classes}, BEST_PATH)\n",
    "        print(\"  ↑ Saved best:\", BEST_PATH.as_posix(), f\"({EARLYSTOP_METRIC}={best_score:.4f})\")\n",
    "    else:\n",
    "        bad_count += 1\n",
    "        if bad_count >= early_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Đánh giá TEST (an toàn hơn với weights_only)\n",
    "ckpt = torch.load(BEST_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "test_metrics = evaluate(test_loader, model, criterion, device, classes)\n",
    "print(\"\\n=== TEST ===\")\n",
    "print(f\"loss={test_metrics['loss']:.4f} | acc={test_metrics['acc']:.3f} | \"\n",
    "      f\"macro-F1={test_metrics['f1']:.3f} | macro-AUC={test_metrics['auc']:.3f}\")\n",
    "print(\"AUC theo lớp:\", test_metrics[\"per_class_auc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c4c89-fb53-4e8f-88ac-6eaf8b04e89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
