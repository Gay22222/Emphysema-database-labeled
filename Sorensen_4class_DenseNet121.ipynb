{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9293f5-c434-4f35-9bba-3beb989c7ddd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_key</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>label_name</th>\n",
       "      <th>label_code</th>\n",
       "      <th>preprocessed_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject24_top</td>\n",
       "      <td>24</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject24_middle</td>\n",
       "      <td>24</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject24_bottom</td>\n",
       "      <td>24</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject9_top</td>\n",
       "      <td>9</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject9_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject9_middle</td>\n",
       "      <td>9</td>\n",
       "      <td>NT</td>\n",
       "      <td>1</td>\n",
       "      <td>Data/processed/sorensen/preprocessed/subject9_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          slice_key  subject_id label_name  label_code  \\\n",
       "0     subject24_top          24         NT           1   \n",
       "1  subject24_middle          24         NT           1   \n",
       "2  subject24_bottom          24         NT           1   \n",
       "3      subject9_top           9         NT           1   \n",
       "4   subject9_middle           9         NT           1   \n",
       "\n",
       "                                   preprocessed_path  \n",
       "0  Data/processed/sorensen/preprocessed/subject24...  \n",
       "1  Data/processed/sorensen/preprocessed/subject24...  \n",
       "2  Data/processed/sorensen/preprocessed/subject24...  \n",
       "3  Data/processed/sorensen/preprocessed/subject9_...  \n",
       "4  Data/processed/sorensen/preprocessed/subject9_...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT   = Path(\"Data\")\n",
    "OUT         = DATA_ROOT / \"processed\" / \"sorensen\"\n",
    "MANIFEST_CLEAN = Path(\"Data/processed/emphysema_all/manifest_emphysema_all.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(MANIFEST_CLEAN)\n",
    "cols_keep = [\"slice_key\",\"subject_id\",\"label_name\",\"label_code\",\"preprocessed_path\"]\n",
    "df = df[cols_keep].copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4e4d6b-1b89-497b-8d35-96513f8f6557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: regular (>=3 PLE subjects). PATH_COL=preprocessed_path\n",
      "Số lượng: {'train': 95, 'val': 38, 'test': 28}\n",
      "Số subject: {'train': 51, 'val': 18, 'test': 16}\n",
      "\n",
      "Phân bố lớp (train):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NT</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLE</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSE</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLE</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "label_name       \n",
       "NT             35\n",
       "CLE            26\n",
       "PSE            20\n",
       "PLE            14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phân bố lớp (val):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NT</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLE</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSE</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLE</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "label_name       \n",
       "NT             21\n",
       "CLE            12\n",
       "PSE             2\n",
       "PLE             3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phân bố lớp (test):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NT</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLE</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSE</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLE</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "label_name       \n",
       "NT              5\n",
       "CLE             5\n",
       "PSE            15\n",
       "PLE             3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2) Split theo subject (grouped) + stratified theo label\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# ---- 0) Cột định danh file/slice dùng để so khớp loại trừ ----\n",
    "PATH_COL = (\n",
    "    \"image_path\" if \"image_path\" in df.columns\n",
    "    else (\"preprocessed_path\" if \"preprocessed_path\" in df.columns\n",
    "          else (\"slice_key\" if \"slice_key\" in df.columns else None))\n",
    ")\n",
    "if PATH_COL is None:\n",
    "    raise ValueError(\"Không tìm thấy cột định danh (image_path / preprocessed_path / slice_key) trong df.\")\n",
    "\n",
    "# ---- 1) Thứ tự classes (nếu chưa có) ----\n",
    "try:\n",
    "    classes\n",
    "except NameError:\n",
    "    classes = [\"NT\",\"CLE\",\"PSE\",\"PLE\"] if {\"NT\",\"CLE\",\"PSE\",\"PLE\"}.issubset(set(df[\"label_name\"].unique())) \\\n",
    "              else sorted(df[\"label_name\"].unique().tolist())\n",
    "\n",
    "# ---- 2) Major label theo subject để stratify ----\n",
    "subject_stats = df.groupby([\"subject_id\",\"label_name\"]).size().unstack(fill_value=0)\n",
    "major_label   = subject_stats.idxmax(axis=1)\n",
    "sub2label     = major_label.to_dict()\n",
    "\n",
    "# Các subject có PLE và số slice PLE theo subject\n",
    "ple_subj         = subject_stats.index[subject_stats.get(\"PLE\", 0) > 0].tolist()\n",
    "ple_cnt_by_subj  = subject_stats.get(\"PLE\", 0)\n",
    "\n",
    "# ---- 3) Helpers ----\n",
    "def safe_n_splits(labels, desired, min_allowed=2):\n",
    "    \"\"\"Giảm n_splits không vượt quá số lượng lớp ít nhất.\"\"\"\n",
    "    vc = pd.Series(labels).value_counts()\n",
    "    return max(min_allowed, min(desired, int(vc.min())))\n",
    "\n",
    "def _move_subject(src, dst, sid):\n",
    "    rows = src[src[\"subject_id\"]==sid]\n",
    "    src  = src[src[\"subject_id\"]!=sid].reset_index(drop=True)\n",
    "    dst  = pd.concat([dst, rows], ignore_index=True)\n",
    "    return src, dst\n",
    "\n",
    "def _save_and_report(train_df, val_df, test_df):\n",
    "    print(\"Số lượng:\", { \"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df) })\n",
    "    print(\"Số subject:\", {\n",
    "        \"train\": train_df[\"subject_id\"].nunique(),\n",
    "        \"val\":   val_df[\"subject_id\"].nunique(),\n",
    "        \"test\":  test_df[\"subject_id\"].nunique()\n",
    "    })\n",
    "    # Lưu CSV chia tập\n",
    "    SPLIT_DIR = OUT / \"splits\"\n",
    "    SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    train_df.to_csv(SPLIT_DIR / \"train.csv\", index=False)\n",
    "    val_df.to_csv(SPLIT_DIR / \"val.csv\", index=False)\n",
    "    test_df.to_csv(SPLIT_DIR / \"test.csv\", index=False)\n",
    "    # Phân bố lớp\n",
    "    print(\"\\nPhân bố lớp (train):\")\n",
    "    display(train_df[\"label_name\"].value_counts().reindex(classes, fill_value=0).to_frame(\"count\"))\n",
    "    print(\"\\nPhân bố lớp (val):\")\n",
    "    display(val_df[\"label_name\"].value_counts().reindex(classes, fill_value=0).to_frame(\"count\"))\n",
    "    print(\"\\nPhân bố lớp (test):\")\n",
    "    display(test_df[\"label_name\"].value_counts().reindex(classes, fill_value=0).to_frame(\"count\"))\n",
    "\n",
    "# =========================\n",
    "# CASE A: >= 3 PLE subjects\n",
    "# =========================\n",
    "groups  = df[\"subject_id\"].values\n",
    "y_major = df[\"subject_id\"].map(sub2label).values\n",
    "\n",
    "if len(ple_subj) >= 3:\n",
    "    n_splits1 = safe_n_splits(y_major, desired=5, min_allowed=2)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits1, shuffle=True, random_state=42)\n",
    "    fold_pairs = list(sgkf.split(df, y=y_major, groups=groups))\n",
    "\n",
    "    test_idx   = fold_pairs[0][1]\n",
    "    remain_idx = fold_pairs[0][0]\n",
    "    df_remain  = df.iloc[remain_idx].copy()\n",
    "\n",
    "    groups_r  = df_remain[\"subject_id\"].values\n",
    "    y_major_r = df_remain[\"subject_id\"].map(sub2label).values\n",
    "    n_splits2 = safe_n_splits(y_major_r, desired=4, min_allowed=2)\n",
    "\n",
    "    sgkf2     = StratifiedGroupKFold(n_splits=n_splits2, shuffle=True, random_state=7)\n",
    "    val_idx_r, train_idx_r = list(sgkf2.split(df_remain, y=y_major_r, groups=groups_r))[0][1], \\\n",
    "                             list(sgkf2.split(df_remain, y=y_major_r, groups=groups_r))[0][0]\n",
    "\n",
    "    train_df = df_remain.iloc[train_idx_r].reset_index(drop=True)\n",
    "    val_df   = df_remain.iloc[val_idx_r].reset_index(drop=True)\n",
    "    test_df  = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Sửa sau chia để đảm bảo mỗi tập có đủ lớp\n",
    "    subj_cls = df.groupby([\"subject_id\",\"label_name\"]).size().unstack(fill_value=0)\n",
    "\n",
    "    # Ưu tiên TEST rồi VAL\n",
    "    for cls in classes:\n",
    "        if cls not in test_df[\"label_name\"].unique():\n",
    "            cands = (subj_cls[subj_cls[cls]>0][cls].sort_values(ascending=False).index.tolist())\n",
    "            cands = [sid for sid in cands if sid in set(train_df[\"subject_id\"].unique())]\n",
    "            if cands:\n",
    "                train_df, test_df = _move_subject(train_df, test_df, cands[0])\n",
    "    for cls in classes:\n",
    "        if cls not in val_df[\"label_name\"].unique():\n",
    "            cands = (subj_cls[subj_cls[cls]>0][cls].sort_values(ascending=False).index.tolist())\n",
    "            cands = [sid for sid in cands if sid in set(train_df[\"subject_id\"].unique())]\n",
    "            if cands:\n",
    "                train_df, val_df = _move_subject(train_df, val_df, cands[0])\n",
    "\n",
    "    # Loại trùng subject\n",
    "    s_test = set(test_df[\"subject_id\"].unique())\n",
    "    s_val  = set(val_df[\"subject_id\"].unique())\n",
    "    train_df = train_df[~train_df[\"subject_id\"].isin(s_test | s_val)].reset_index(drop=True)\n",
    "    val_df   = val_df[~val_df[\"subject_id\"].isin(s_test)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Mode: regular (>=3 PLE subjects). PATH_COL={PATH_COL}\")\n",
    "    _save_and_report(train_df, val_df, test_df)\n",
    "\n",
    "# =========================\n",
    "# CASE B: chỉ có 2 PLE subjects\n",
    "# =========================\n",
    "else:\n",
    "    print(f\"Mode: SPECIAL (only 2 PLE subjects). PATH_COL={PATH_COL}\")\n",
    "\n",
    "    # Chọn subject PLE có ít slice hơn cho TEST để giữ nhiều PLE cho train/val\n",
    "    ple_subj_sorted = sorted(ple_subj, key=lambda s: int(ple_cnt_by_subj.loc[s]))\n",
    "    ple_test_sid, ple_train_sid = ple_subj_sorted[0], ple_subj_sorted[1]\n",
    "\n",
    "    # 1) TEST: trọn subject ple_test_sid + thêm fold từ phần còn lại\n",
    "    test_df_ple = df[df[\"subject_id\"]==ple_test_sid]\n",
    "    df_non_test = df[df[\"subject_id\"]!=ple_test_sid].reset_index(drop=True)\n",
    "\n",
    "    groups_nt  = df_non_test[\"subject_id\"].values\n",
    "    y_major_nt = df_non_test[\"subject_id\"].map(sub2label).values\n",
    "    n_splits_nt = safe_n_splits(y_major_nt, desired=5, min_allowed=2)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits_nt, shuffle=True, random_state=42)\n",
    "    folds = list(sgkf.split(df_non_test, y=y_major_nt, groups=groups_nt))\n",
    "    extra_test_idx = folds[0][1]\n",
    "\n",
    "    test_df_rest = df_non_test.iloc[extra_test_idx].reset_index(drop=True)\n",
    "    test_df      = pd.concat([test_df_ple, test_df_rest], ignore_index=True)\n",
    "\n",
    "    # 2) Pool còn lại cho TRAIN/VAL (loại bỏ các hàng đã vào TEST theo PATH_COL)\n",
    "    df_pool = df[~df[PATH_COL].isin(test_df[PATH_COL])].reset_index(drop=True)\n",
    "\n",
    "    # 3) Với subject PLE còn lại: CHIA THEO SLICE cho VAL (>=1) và giữ LẠI >=1 cho TRAIN nếu có thể\n",
    "    pool_ple  = df_pool[df_pool[\"subject_id\"]==ple_train_sid].reset_index(drop=True)\n",
    "    pool_rest = df_pool[df_pool[\"subject_id\"]!=ple_train_sid].reset_index(drop=True)\n",
    "\n",
    "    # số slice PLE còn lại (chỉ 1 subject)\n",
    "    n_ple_total = len(pool_ple)\n",
    "    # chọn số slice cho VAL: tối thiểu 1, tối đa n_ple_total-1 (để còn lại >=1 cho TRAIN nếu n_ple_total>1)\n",
    "    n_val_ple = 1 if n_ple_total >= 1 else 0\n",
    "    if n_ple_total > 1:\n",
    "        n_val_ple = min(max(1, int(round(n_ple_total * 0.15))), n_ple_total - 1)\n",
    "    # (nếu chỉ có đúng 1 slice thì val = 1, train = 0 – trường hợp bất khả kháng)\n",
    "\n",
    "    if n_val_ple > 0:\n",
    "        rng = np.random.RandomState(123)\n",
    "        val_ple_idx = rng.choice(n_ple_total, size=n_val_ple, replace=False)\n",
    "        val_ple_df  = pool_ple.iloc[val_ple_idx]\n",
    "        train_ple_df= pool_ple.drop(pool_ple.index[val_ple_idx])\n",
    "    else:\n",
    "        val_ple_df  = pool_ple.iloc[:0]\n",
    "        train_ple_df= pool_ple.copy()\n",
    "\n",
    "    # 4) Non-PLE: chia theo subject cho TRAIN/VAL\n",
    "    groups_r  = pool_rest[\"subject_id\"].values\n",
    "    y_major_r = pool_rest[\"subject_id\"].map(sub2label).values\n",
    "    n_splits2 = safe_n_splits(y_major_r, desired=4, min_allowed=2)\n",
    "    sgkf2     = StratifiedGroupKFold(n_splits=n_splits2, shuffle=True, random_state=7)\n",
    "    _split    = list(sgkf2.split(pool_rest, y=y_major_r, groups=groups_r))[0]\n",
    "    val_idx_r, train_idx_r = _split[1], _split[0]\n",
    "\n",
    "    val_rest_df   = pool_rest.iloc[val_idx_r].reset_index(drop=True)\n",
    "    train_rest_df = pool_rest.iloc[train_idx_r].reset_index(drop=True)\n",
    "\n",
    "    # GHÉP: cho phép trùng subject ple_train_sid giữa TRAIN và VAL (slice-level split)\n",
    "    val_df   = pd.concat([val_rest_df,   val_ple_df],   ignore_index=True)\n",
    "    train_df = pd.concat([train_rest_df, train_ple_df], ignore_index=True)\n",
    "\n",
    "    # Loại trùng subject với TEST (đảm bảo disjoint theo subject so với TEST)\n",
    "    s_test = set(test_df[\"subject_id\"].unique())\n",
    "    train_df = train_df[~train_df[\"subject_id\"].isin(s_test)].reset_index(drop=True)\n",
    "    val_df   = val_df[~val_df[\"subject_id\"].isin(s_test)].reset_index(drop=True)\n",
    "\n",
    "    # KHÔNG loại trùng subject giữa TRAIN và VAL đối với ple_train_sid (cho phép overlap slice-level)\n",
    "    # Nếu có subject trùng khác ngoài ple_train_sid (hiếm), loại khỏi TRAIN để tránh leakage\n",
    "    overlap = (set(train_df[\"subject_id\"].unique()) & set(val_df[\"subject_id\"].unique())) - {ple_train_sid}\n",
    "    if overlap:\n",
    "        train_df = train_df[~train_df[\"subject_id\"].isin(overlap)].reset_index(drop=True)\n",
    "\n",
    "    # Cảnh báo leakage ở VAL do lấy slice từ subject PLE còn lại\n",
    "    print(f\"WARNING: VAL chứa {(val_df['label_name']=='PLE').sum()} slice PLE từ subject {ple_train_sid}. \"\n",
    "          \"Chỉ dùng VAL cho early-stopping; đánh giá tổng quát PLE dựa trên TEST.\")\n",
    "\n",
    "    _save_and_report(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa7939a-83a8-48c1-9a8f-89c776f3bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NT': 0.6785714285714286,\n",
       " 'CLE': 0.9134615384615384,\n",
       " 'PSE': 1.1875,\n",
       " 'PLE': 1.6964285714285714}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Class weights theo phân bố train\n",
    "from collections import Counter\n",
    "\n",
    "classes = [\"NT\",\"CLE\",\"PSE\",\"PLE\"]  \n",
    "cnt = Counter(train_df[\"label_name\"])\n",
    "N = sum(cnt.values()); K = len(classes)\n",
    "class_weights = {c: (N / (K * cnt.get(c,1))) for c in classes}\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb0b491-5ade-4901-835d-b4a1ed5e5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dataset + Augmentation\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "\n",
    "label2id = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "train_tfms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05,\n",
    "                       rotate_limit=8, border_mode=0, value=0, p=0.5),\n",
    "    A.ElasticTransform(alpha=10, sigma=10*0.05, alpha_affine=5,\n",
    "                       border_mode=0, value=0, p=0.15),\n",
    "], p=1.0)\n",
    "\n",
    "val_tfms = A.Compose([], p=1.0)\n",
    "\n",
    "class SorensenNPZDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        z = np.load(row[\"preprocessed_path\"], allow_pickle=True)\n",
    "        img = z[\"img_norm\"].astype(np.float32)  # [H,W] in [0,1]\n",
    "        \n",
    "\n",
    "        # Albumentations nhận HxWxc -> thêm kênh giả\n",
    "        img3 = np.expand_dims(img, axis=2)\n",
    "        if self.transforms is not None:\n",
    "            img3 = self.transforms(image=img3)[\"image\"]\n",
    "        img = img3[...,0]  # quay lại HxW\n",
    "\n",
    "        # To tensor (1,H,W)\n",
    "        x = torch.from_numpy(img).unsqueeze(0)        # 1 channel\n",
    "        y = torch.tensor(label2id[row[\"label_name\"]], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d47a433-1595-474f-8669-3ceaa03dcce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6786, 0.9135, 1.1875, 1.6964])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) DataLoader\n",
    "import os\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0  # max(0, min(4, os.cpu_count() // 2))\n",
    "\n",
    "train_ds = SorensenNPZDataset(train_df, transforms=train_tfms)\n",
    "val_ds   = SorensenNPZDataset(val_df,   transforms=val_tfms)\n",
    "test_ds  = SorensenNPZDataset(test_df,  transforms=val_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Class weights tensor cho CrossEntropy\n",
    "weights_tensor = torch.tensor([class_weights[c] for c in classes], dtype=torch.float32)\n",
    "weights_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0483449a-e9fe-4274-8da1-6bf3c92cab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "DenseNet                                 [1, 1, 512, 512]          [1, 4]                    --\n",
       "├─Sequential: 1-1                        [1, 1, 512, 512]          [1, 1024, 16, 16]         --\n",
       "│    └─Conv2d: 2-1                       [1, 1, 512, 512]          [1, 64, 256, 256]         3,136\n",
       "│    └─BatchNorm2d: 2-2                  [1, 64, 256, 256]         [1, 64, 256, 256]         128\n",
       "│    └─ReLU: 2-3                         [1, 64, 256, 256]         [1, 64, 256, 256]         --\n",
       "│    └─MaxPool2d: 2-4                    [1, 64, 256, 256]         [1, 64, 128, 128]         --\n",
       "│    └─_DenseBlock: 2-5                  [1, 64, 128, 128]         [1, 256, 128, 128]        --\n",
       "│    │    └─_DenseLayer: 3-1             [1, 64, 128, 128]         [1, 32, 128, 128]         45,440\n",
       "│    │    └─_DenseLayer: 3-2             [1, 64, 128, 128]         [1, 32, 128, 128]         49,600\n",
       "│    │    └─_DenseLayer: 3-3             [1, 64, 128, 128]         [1, 32, 128, 128]         53,760\n",
       "│    │    └─_DenseLayer: 3-4             [1, 64, 128, 128]         [1, 32, 128, 128]         57,920\n",
       "│    │    └─_DenseLayer: 3-5             [1, 64, 128, 128]         [1, 32, 128, 128]         62,080\n",
       "│    │    └─_DenseLayer: 3-6             [1, 64, 128, 128]         [1, 32, 128, 128]         66,240\n",
       "│    └─_Transition: 2-6                  [1, 256, 128, 128]        [1, 128, 64, 64]          --\n",
       "│    │    └─BatchNorm2d: 3-7             [1, 256, 128, 128]        [1, 256, 128, 128]        512\n",
       "│    │    └─ReLU: 3-8                    [1, 256, 128, 128]        [1, 256, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-9                  [1, 256, 128, 128]        [1, 128, 128, 128]        32,768\n",
       "│    │    └─AvgPool2d: 3-10              [1, 128, 128, 128]        [1, 128, 64, 64]          --\n",
       "│    └─_DenseBlock: 2-7                  [1, 128, 64, 64]          [1, 512, 64, 64]          --\n",
       "│    │    └─_DenseLayer: 3-11            [1, 128, 64, 64]          [1, 32, 64, 64]           53,760\n",
       "│    │    └─_DenseLayer: 3-12            [1, 128, 64, 64]          [1, 32, 64, 64]           57,920\n",
       "│    │    └─_DenseLayer: 3-13            [1, 128, 64, 64]          [1, 32, 64, 64]           62,080\n",
       "│    │    └─_DenseLayer: 3-14            [1, 128, 64, 64]          [1, 32, 64, 64]           66,240\n",
       "│    │    └─_DenseLayer: 3-15            [1, 128, 64, 64]          [1, 32, 64, 64]           70,400\n",
       "│    │    └─_DenseLayer: 3-16            [1, 128, 64, 64]          [1, 32, 64, 64]           74,560\n",
       "│    │    └─_DenseLayer: 3-17            [1, 128, 64, 64]          [1, 32, 64, 64]           78,720\n",
       "│    │    └─_DenseLayer: 3-18            [1, 128, 64, 64]          [1, 32, 64, 64]           82,880\n",
       "│    │    └─_DenseLayer: 3-19            [1, 128, 64, 64]          [1, 32, 64, 64]           87,040\n",
       "│    │    └─_DenseLayer: 3-20            [1, 128, 64, 64]          [1, 32, 64, 64]           91,200\n",
       "│    │    └─_DenseLayer: 3-21            [1, 128, 64, 64]          [1, 32, 64, 64]           95,360\n",
       "│    │    └─_DenseLayer: 3-22            [1, 128, 64, 64]          [1, 32, 64, 64]           99,520\n",
       "│    └─_Transition: 2-8                  [1, 512, 64, 64]          [1, 256, 32, 32]          --\n",
       "│    │    └─BatchNorm2d: 3-23            [1, 512, 64, 64]          [1, 512, 64, 64]          1,024\n",
       "│    │    └─ReLU: 3-24                   [1, 512, 64, 64]          [1, 512, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-25                 [1, 512, 64, 64]          [1, 256, 64, 64]          131,072\n",
       "│    │    └─AvgPool2d: 3-26              [1, 256, 64, 64]          [1, 256, 32, 32]          --\n",
       "│    └─_DenseBlock: 2-9                  [1, 256, 32, 32]          [1, 1024, 32, 32]         --\n",
       "│    │    └─_DenseLayer: 3-27            [1, 256, 32, 32]          [1, 32, 32, 32]           70,400\n",
       "│    │    └─_DenseLayer: 3-28            [1, 256, 32, 32]          [1, 32, 32, 32]           74,560\n",
       "│    │    └─_DenseLayer: 3-29            [1, 256, 32, 32]          [1, 32, 32, 32]           78,720\n",
       "│    │    └─_DenseLayer: 3-30            [1, 256, 32, 32]          [1, 32, 32, 32]           82,880\n",
       "│    │    └─_DenseLayer: 3-31            [1, 256, 32, 32]          [1, 32, 32, 32]           87,040\n",
       "│    │    └─_DenseLayer: 3-32            [1, 256, 32, 32]          [1, 32, 32, 32]           91,200\n",
       "│    │    └─_DenseLayer: 3-33            [1, 256, 32, 32]          [1, 32, 32, 32]           95,360\n",
       "│    │    └─_DenseLayer: 3-34            [1, 256, 32, 32]          [1, 32, 32, 32]           99,520\n",
       "│    │    └─_DenseLayer: 3-35            [1, 256, 32, 32]          [1, 32, 32, 32]           103,680\n",
       "│    │    └─_DenseLayer: 3-36            [1, 256, 32, 32]          [1, 32, 32, 32]           107,840\n",
       "│    │    └─_DenseLayer: 3-37            [1, 256, 32, 32]          [1, 32, 32, 32]           112,000\n",
       "│    │    └─_DenseLayer: 3-38            [1, 256, 32, 32]          [1, 32, 32, 32]           116,160\n",
       "│    │    └─_DenseLayer: 3-39            [1, 256, 32, 32]          [1, 32, 32, 32]           120,320\n",
       "│    │    └─_DenseLayer: 3-40            [1, 256, 32, 32]          [1, 32, 32, 32]           124,480\n",
       "│    │    └─_DenseLayer: 3-41            [1, 256, 32, 32]          [1, 32, 32, 32]           128,640\n",
       "│    │    └─_DenseLayer: 3-42            [1, 256, 32, 32]          [1, 32, 32, 32]           132,800\n",
       "│    │    └─_DenseLayer: 3-43            [1, 256, 32, 32]          [1, 32, 32, 32]           136,960\n",
       "│    │    └─_DenseLayer: 3-44            [1, 256, 32, 32]          [1, 32, 32, 32]           141,120\n",
       "│    │    └─_DenseLayer: 3-45            [1, 256, 32, 32]          [1, 32, 32, 32]           145,280\n",
       "│    │    └─_DenseLayer: 3-46            [1, 256, 32, 32]          [1, 32, 32, 32]           149,440\n",
       "│    │    └─_DenseLayer: 3-47            [1, 256, 32, 32]          [1, 32, 32, 32]           153,600\n",
       "│    │    └─_DenseLayer: 3-48            [1, 256, 32, 32]          [1, 32, 32, 32]           157,760\n",
       "│    │    └─_DenseLayer: 3-49            [1, 256, 32, 32]          [1, 32, 32, 32]           161,920\n",
       "│    │    └─_DenseLayer: 3-50            [1, 256, 32, 32]          [1, 32, 32, 32]           166,080\n",
       "│    └─_Transition: 2-10                 [1, 1024, 32, 32]         [1, 512, 16, 16]          --\n",
       "│    │    └─BatchNorm2d: 3-51            [1, 1024, 32, 32]         [1, 1024, 32, 32]         2,048\n",
       "│    │    └─ReLU: 3-52                   [1, 1024, 32, 32]         [1, 1024, 32, 32]         --\n",
       "│    │    └─Conv2d: 3-53                 [1, 1024, 32, 32]         [1, 512, 32, 32]          524,288\n",
       "│    │    └─AvgPool2d: 3-54              [1, 512, 32, 32]          [1, 512, 16, 16]          --\n",
       "│    └─_DenseBlock: 2-11                 [1, 512, 16, 16]          [1, 1024, 16, 16]         --\n",
       "│    │    └─_DenseLayer: 3-55            [1, 512, 16, 16]          [1, 32, 16, 16]           103,680\n",
       "│    │    └─_DenseLayer: 3-56            [1, 512, 16, 16]          [1, 32, 16, 16]           107,840\n",
       "│    │    └─_DenseLayer: 3-57            [1, 512, 16, 16]          [1, 32, 16, 16]           112,000\n",
       "│    │    └─_DenseLayer: 3-58            [1, 512, 16, 16]          [1, 32, 16, 16]           116,160\n",
       "│    │    └─_DenseLayer: 3-59            [1, 512, 16, 16]          [1, 32, 16, 16]           120,320\n",
       "│    │    └─_DenseLayer: 3-60            [1, 512, 16, 16]          [1, 32, 16, 16]           124,480\n",
       "│    │    └─_DenseLayer: 3-61            [1, 512, 16, 16]          [1, 32, 16, 16]           128,640\n",
       "│    │    └─_DenseLayer: 3-62            [1, 512, 16, 16]          [1, 32, 16, 16]           132,800\n",
       "│    │    └─_DenseLayer: 3-63            [1, 512, 16, 16]          [1, 32, 16, 16]           136,960\n",
       "│    │    └─_DenseLayer: 3-64            [1, 512, 16, 16]          [1, 32, 16, 16]           141,120\n",
       "│    │    └─_DenseLayer: 3-65            [1, 512, 16, 16]          [1, 32, 16, 16]           145,280\n",
       "│    │    └─_DenseLayer: 3-66            [1, 512, 16, 16]          [1, 32, 16, 16]           149,440\n",
       "│    │    └─_DenseLayer: 3-67            [1, 512, 16, 16]          [1, 32, 16, 16]           153,600\n",
       "│    │    └─_DenseLayer: 3-68            [1, 512, 16, 16]          [1, 32, 16, 16]           157,760\n",
       "│    │    └─_DenseLayer: 3-69            [1, 512, 16, 16]          [1, 32, 16, 16]           161,920\n",
       "│    │    └─_DenseLayer: 3-70            [1, 512, 16, 16]          [1, 32, 16, 16]           166,080\n",
       "│    └─BatchNorm2d: 2-12                 [1, 1024, 16, 16]         [1, 1024, 16, 16]         2,048\n",
       "├─Linear: 1-2                            [1, 1024]                 [1, 4]                    4,100\n",
       "===================================================================================================================\n",
       "Total params: 6,951,684\n",
       "Trainable params: 6,951,684\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 14.39\n",
       "===================================================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 943.19\n",
       "Params size (MB): 27.81\n",
       "Estimated Total Size (MB): 972.05\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model: DenseNet121\n",
    "import torch, torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "classes = [\"NT\",\"CLE\",\"PSE\",\"PLE\"]\n",
    "\n",
    "def build_densenet121(num_classes=4, in_ch=1):\n",
    "    model = models.densenet121(\n",
    "        weights=models.DenseNet121_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    if in_ch == 1:\n",
    "        old_conv = model.features.conv0\n",
    "        model.features.conv0 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=old_conv.out_channels,\n",
    "            kernel_size=old_conv.kernel_size,\n",
    "            stride=old_conv.stride,\n",
    "            padding=old_conv.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = build_densenet121(num_classes=len(classes), in_ch=1).to(device)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 1, 512, 512),\n",
    "        col_names=(\"input_size\",\"output_size\",\"num_params\"),\n",
    "        depth=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44292d99-2b16-483c-861f-07d22eb0b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# dùng CrossEntropy với class_weights\n",
    "# Loss/optim\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights_tensor.to(device),\n",
    "    label_smoothing=0.05\n",
    ")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "BEST_PATH = OUT / \"densenet121_best.pth\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca117d43-6d1c-4fe7-8057-94fc51abc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model, criterion, device, classes):\n",
    "    model.eval()\n",
    "    tot_loss, n = 0.0, 0\n",
    "    probs_all, y_all = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tot_loss += loss.item() * x.size(0); n += x.size(0)\n",
    "        probs_all.append(logits.softmax(1).cpu().numpy())\n",
    "        y_all.append(y.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_all)\n",
    "    y_prob = np.concatenate(probs_all, axis=0)\n",
    "    y_pred = y_prob.argmax(1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    # AUC OvR theo lớp khả dụng (không NaN)\n",
    "    per_class_auc, auc_vals = {}, []\n",
    "    C = len(classes)\n",
    "    for i, cls in enumerate(classes):\n",
    "        y_bin = (y_true == i).astype(int)\n",
    "        if y_bin.min() == y_bin.max():  # thiếu lớp i ở tập này\n",
    "            per_class_auc[cls] = np.nan\n",
    "        else:\n",
    "            auc_i = roc_auc_score(y_bin, y_prob[:, i])\n",
    "            per_class_auc[cls] = float(auc_i); auc_vals.append(auc_i)\n",
    "    macro_auc = float(np.mean(auc_vals)) if len(auc_vals) > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"loss\": tot_loss / max(n, 1),\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": macro_auc,\n",
    "        \"per_class_auc\": per_class_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb87a71-7c1a-423f-8081-2a6891037df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/30 - loss: 1.4415 - val_loss: 1.1915 - val_acc: 0.3947 - val_f1: 0.2073 - val_auc: 0.5502\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.5502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/30 - loss: 1.3180 - val_loss: 1.4369 - val_acc: 0.1053 - val_f1: 0.0870 - val_auc: 0.3581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/30 - loss: 1.1475 - val_loss: 1.4957 - val_acc: 0.1579 - val_f1: 0.1443 - val_auc: 0.5154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/30 - loss: 1.0290 - val_loss: 1.5777 - val_acc: 0.1316 - val_f1: 0.1208 - val_auc: 0.5580\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.5580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/30 - loss: 1.0039 - val_loss: 1.5533 - val_acc: 0.2632 - val_f1: 0.2220 - val_auc: 0.6265\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.6265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/30 - loss: 0.9615 - val_loss: 1.0952 - val_acc: 0.6316 - val_f1: 0.4794 - val_auc: 0.7898\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.7898)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07/30 - loss: 0.8305 - val_loss: 0.9358 - val_acc: 0.7368 - val_f1: 0.5578 - val_auc: 0.8400\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.8400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/30 - loss: 0.7631 - val_loss: 1.0423 - val_acc: 0.6579 - val_f1: 0.5280 - val_auc: 0.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09/30 - loss: 0.7575 - val_loss: 0.9997 - val_acc: 0.6579 - val_f1: 0.5073 - val_auc: 0.8190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - loss: 0.6959 - val_loss: 1.0529 - val_acc: 0.5789 - val_f1: 0.5139 - val_auc: 0.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - loss: 0.6439 - val_loss: 0.9741 - val_acc: 0.6842 - val_f1: 0.5441 - val_auc: 0.8269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - loss: 0.6795 - val_loss: 1.0162 - val_acc: 0.6316 - val_f1: 0.4879 - val_auc: 0.8221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - loss: 0.6622 - val_loss: 0.9767 - val_acc: 0.6579 - val_f1: 0.5199 - val_auc: 0.8374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - loss: 0.6469 - val_loss: 1.0043 - val_acc: 0.6316 - val_f1: 0.4962 - val_auc: 0.8513\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.8513)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - loss: 0.6129 - val_loss: 0.9343 - val_acc: 0.6842 - val_f1: 0.5526 - val_auc: 0.8616\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.8616)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - loss: 0.6334 - val_loss: 0.9631 - val_acc: 0.6579 - val_f1: 0.5284 - val_auc: 0.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - loss: 0.6273 - val_loss: 0.9715 - val_acc: 0.6579 - val_f1: 0.5171 - val_auc: 0.8513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - loss: 0.5689 - val_loss: 0.9457 - val_acc: 0.6579 - val_f1: 0.5284 - val_auc: 0.8519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - loss: 0.5999 - val_loss: 0.9138 - val_acc: 0.7105 - val_f1: 0.5626 - val_auc: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - loss: 0.6035 - val_loss: 0.9368 - val_acc: 0.6842 - val_f1: 0.5526 - val_auc: 0.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - loss: 0.6589 - val_loss: 0.8766 - val_acc: 0.6842 - val_f1: 0.5582 - val_auc: 0.8680\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.8680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - loss: 0.6117 - val_loss: 1.1296 - val_acc: 0.5789 - val_f1: 0.4804 - val_auc: 0.8268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - loss: 0.5880 - val_loss: 0.9750 - val_acc: 0.6842 - val_f1: 0.5482 - val_auc: 0.7809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - loss: 0.4987 - val_loss: 0.7952 - val_acc: 0.7105 - val_f1: 0.5218 - val_auc: 0.9700\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.9700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - loss: 0.5701 - val_loss: 0.9066 - val_acc: 0.6316 - val_f1: 0.5389 - val_auc: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - loss: 0.5833 - val_loss: 0.9871 - val_acc: 0.6579 - val_f1: 0.5690 - val_auc: 0.9584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - loss: 0.5243 - val_loss: 0.8822 - val_acc: 0.7105 - val_f1: 0.5453 - val_auc: 0.9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - loss: 0.5601 - val_loss: 1.3200 - val_acc: 0.4737 - val_f1: 0.4881 - val_auc: 0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - loss: 0.4640 - val_loss: 0.7612 - val_acc: 0.6842 - val_f1: 0.5548 - val_auc: 0.9067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - loss: 0.5470 - val_loss: 0.9356 - val_acc: 0.6316 - val_f1: 0.5863 - val_auc: 0.9804\n",
      "  ↑ Saved best: Data/processed/sorensen/cnn_baseline_best.pth (auc=0.9804)\n",
      "\n",
      "=== TEST ===\n",
      "loss=1.3769 | acc=0.464 | macro-F1=0.494 | macro-AUC=0.901\n",
      "AUC theo lớp: {'NT': 0.9565217391304348, 'CLE': 0.8347826086956522, 'PSE': 0.8256410256410256, 'PLE': 0.9866666666666666}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Cấu hình\n",
    "EPOCHS = 30                      \n",
    "BEST_PATH = OUT / \"densenet121_best.pth\"\n",
    "EARLYSTOP_METRIC = \"auc\"         # \"auc\" hoặc \"f1\"\n",
    "early_patience = 7\n",
    "\n",
    "# AMP (API mới, sửa cảnh báo)\n",
    "scaler = GradScaler('cuda' if device.type=='cuda' else 'cpu')\n",
    "\n",
    "best_score, bad_count = -1.0, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running, n_seen = 0.0, 0\n",
    "\n",
    "    pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False, ncols=100)\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast('cuda' if device.type=='cuda' else 'cpu'):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        running += loss.item() * x.size(0); n_seen += x.size(0)\n",
    "        pbar.set_postfix_str(f\"loss={running/max(n_seen,1):.4f}\")\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    train_loss = running / max(n_seen, 1)\n",
    "    val_metrics = evaluate(val_loader, model, criterion, device, classes)\n",
    "    scheduler.step()\n",
    "\n",
    "    # log tóm tắt \n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} \"\n",
    "          f\"- loss: {train_loss:.4f} \"\n",
    "          f\"- val_loss: {val_metrics['loss']:.4f} \"\n",
    "          f\"- val_acc: {val_metrics['acc']:.4f} \"\n",
    "          f\"- val_f1: {val_metrics['f1']:.4f} \"\n",
    "          f\"- val_auc: {val_metrics['auc']:.4f}\")\n",
    "\n",
    "    # Early stopping theo AUC (fallback F1 nếu AUC nan)\n",
    "    es_metric = (val_metrics[\"auc\"] if (EARLYSTOP_METRIC==\"auc\" and not np.isnan(val_metrics[\"auc\"]))\n",
    "                 else val_metrics[\"f1\"])\n",
    "    if es_metric > best_score + 1e-4:\n",
    "        best_score = es_metric; bad_count = 0\n",
    "        torch.save({\"model\": model.state_dict(), \"classes\": classes}, BEST_PATH)\n",
    "        print(\"  ↑ Saved best:\", BEST_PATH.as_posix(), f\"({EARLYSTOP_METRIC}={best_score:.4f})\")\n",
    "    else:\n",
    "        bad_count += 1\n",
    "        if bad_count >= early_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Đánh giá TEST (an toàn hơn với weights_only)\n",
    "ckpt = torch.load(BEST_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "test_metrics = evaluate(test_loader, model, criterion, device, classes)\n",
    "print(\"\\n=== TEST ===\")\n",
    "print(f\"loss={test_metrics['loss']:.4f} | acc={test_metrics['acc']:.3f} | \"\n",
    "      f\"macro-F1={test_metrics['f1']:.3f} | macro-AUC={test_metrics['auc']:.3f}\")\n",
    "print(\"AUC theo lớp:\", test_metrics[\"per_class_auc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f992a-4dc7-43b1-8c6b-e2891f15dc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
